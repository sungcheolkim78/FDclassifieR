library(kknn)
cl   <- specClust(clustering.data, centers=8, nn=50, iter.max=100)
cl
plot(PrincipalComponent1, PrincipalComponent2, col=cl$cluster)
table(cl$cluster, yeast$LocalizationSite)
library(kknn)
cl   <- specClust(clustering.data, centers=8, nn=50, iter.max=100)
cl
plot(PrincipalComponent1, PrincipalComponent2, col=cl$cluster)
table(cl$cluster, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(cl$cluster),mean)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
clusterCut
table(clusterCut, yeast$LocalizationSite)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
clusterCut
table(clusterCut, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(clusterCut),mean)
plot(PrincipalComponent1, PrincipalComponent2, col=clusterCut)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
#clusterCut
table(clusterCut, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(clusterCut),mean)
plot(PrincipalComponent1, PrincipalComponent2, col=clusterCut)
library(kknn)
cl   <- specClust(clustering.data, centers=8, nn=50, iter.max=100)
#cl
plot(PrincipalComponent1, PrincipalComponent2, col=cl$cluster)
table(cl$cluster, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(cl$cluster),mean)
set.seed(100)
km <- kmeans(clustering.data, 8, iter.max = 30, nstart=30)
#km
km$cluster
plot(PrincipalComponent1, PrincipalComponent2, col=km$cluster)
points(km$centers, pch=16)
aggregate(tmp[, 2:9],by=list(km$cluster),mean)
table(km$cluster, tmp$LocalizationSite)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.table('yeast.csv')
names(tmp)<- c("SequenceName", "mcg", "gvh", "alm",
"mit", "erl", "pox", "vac", "nuc", "LocalizationSite")
#head(tmp)
#table(tmp$LocalizationSite)
# choose only 'CYT' and 'NUC', ignore SequnceName
Yeast <- tmp[tmp$LocalizationSite %in% c('CYT', 'NUC'), 2:10]
names(Yeast)[ncol(Yeast)] <- 'Site'
Yeast$Site <- factor(Yeast$Site, c('CYT', 'NUC'))
inTraining0 <- createDataPartition(Yeast$Site, p = .75, list = FALSE)
training <- Yeast[ inTraining0,]
testing  <- Yeast[-inTraining0,]
testingY <- as_label(Yeast[-inTraining0, ncol(Yeast)])
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.table('data/agaricus-lepiota.data')
tmp_names <- read.table('data/agaricus-lepiota.names')
tmp <- read.csv('data/agaricus-lepiota.data')
tmp_names <- c('cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor',
'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',
'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',
'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat')
names(tmp)<- tmp_names
tmp <- read.csv('data/agaricus-lepiota.data')
tmp_names <- c('class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor',
'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',
'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',
'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat')
names(tmp)<- tmp_names
# choose only 'CYT' and 'NUC', ignore SequnceName
Mushroom <- tmp[,2:ncol(tmp)]
Mushroom$Class <- ifelse(tmp[,1] == 'e', 'edible', 'poisonous')
inTraining0 <- createDataPartition(Mushroom$Class, p = .75, list = FALSE)
training <- Mushroom[ inTraining0,]
testing  <- Mushroom[-inTraining0,]
testingY <- as_label(Mushroom[-inTraining0, ncol(Mushroom)])
# choose only 'CYT' and 'NUC', ignore SequnceName
Mushroom <- tmp[,2:ncol(tmp)]
Mushroom$Class <- as.factor(ifelse(tmp[,1] == 'e', 'edible', 'poisonous'))
inTraining0 <- createDataPartition(Mushroom$Class, p = .75, list = FALSE)
training <- Mushroom[ inTraining0,]
testing  <- Mushroom[-inTraining0,]
testingY <- as_label(Mushroom[-inTraining0, ncol(Mushroom)])
table(Mushroom[ inTraining0, ncol(Mushroom)])
pca <- princomp(Mushroom[, -23], cor=T) # principal components analysis using correlation matrix
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Class~., training, update=F) %>%
predict(newdata=testing)
devtools::load_all(".")
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Class~., training, update=F) %>%
predict(newdata=testing)
# choose only 'CYT' and 'NUC', ignore SequnceName
Mushroom <- tmp[,2:ncol(tmp)]
Mushroom <- Mushroom[, -`veil-type`]
# choose only 'CYT' and 'NUC', ignore SequnceName
Mushroom <- tmp[,2:ncol(tmp)]
Mushroom <- Mushroom[, -16]
Mushroom$Class <- as.factor(ifelse(tmp[,1] == 'e', 'edible', 'poisonous'))
inTraining0 <- createDataPartition(Mushroom$Class, p = .75, list = FALSE)
training <- Mushroom[ inTraining0,]
testing  <- Mushroom[-inTraining0,]
testingY <- as_label(Mushroom[-inTraining0, ncol(Mushroom)])
devtools::load_all(".")
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Class~., training, update=F) %>%
predict(newdata=testing)
?read.csv
tmp <- read.csv('data/agaricus-lepiota.data', na.strings = '?')
tmp_names <- c('class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor',
'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',
'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',
'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat')
names(tmp)<- tmp_names
# choose only 'CYT' and 'NUC', ignore SequnceName
Mushroom <- tmp[,2:ncol(tmp)]
Mushroom <- Mushroom[, -16]
Mushroom$Class <- as.factor(ifelse(tmp[,1] == 'e', 'edible', 'poisonous'))
inTraining0 <- createDataPartition(Mushroom$Class, p = .75, list = FALSE)
training <- Mushroom[ inTraining0,]
testing  <- Mushroom[-inTraining0,]
testingY <- as_label(Mushroom[-inTraining0, ncol(Mushroom)])
table(Mushroom[ inTraining0, ncol(Mushroom)])
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Class~., training, update=F) %>%
predict(newdata=testing)
testingY
Mushroom[,1]
devtools::load_all(".")
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
Bank <- read.csv('bank.csv', sep=';')
Bank <- read.csv('data/bank.csv', sep=';')
inTraining0 <- createDataPartition(Bank$y, p = .75, list = FALSE)
training <- Bank[ inTraining0,]
testing  <- Bank[-inTraining0,]
testingY <- as_label(Bank[-inTraining0, ncol(Bank)])
table(Bank[ inTraining0, ncol(Bank)])
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(y~., training, update=F) %>%
predict(newdata=testing)
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(y~., training, update=F) %>%
predict(newdata=testing)
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm')) %>%
train(y~., training, update=F)
t1 <- t1 %>%
addmodel.mtrainer(c('svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(y~., training, update=F) %>%
predict(newdata=testingY)
t1 <- t1 %>%
addmodel.mtrainer(c('svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(y~., training, update=F) %>%
predict(newdata=testingY)
t1 <- t1 %>%
addmodel.mtrainer(c('svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(y~., training, update=F)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
tag <- read.csv("tag_data.csv", row.names = 1)
plot(t1)
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
fde1 <- fde(t1$predictions, testingY)
plot_single(fde1, 'score')
store.mtrainer(t1, 'bank_m8_pre.RData')
saveRDS(testingY, 'bank_m8_y.RData')
saveRDS(t1, 'bank_all.RData')
table(Bank$y)
install.packages("kknn")
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.table('yeast.csv')
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.table('data/yeast.csv')
names(tmp)<- c("SequenceName", "mcg", "gvh", "alm",
"mit", "erl", "pox", "vac", "nuc", "LocalizationSite")
#head(tmp)
#table(tmp$LocalizationSite)
# choose only 'CYT' and 'NUC', ignore SequnceName
Yeast <- tmp[tmp$LocalizationSite %in% c('CYT', 'NUC'), 2:10]
names(Yeast)[ncol(Yeast)] <- 'Site'
Yeast$Site <- factor(Yeast$Site, c('CYT', 'NUC'))
inTraining0 <- createDataPartition(Yeast$Site, p = .75, list = FALSE)
training <- Yeast[ inTraining0,]
testing  <- Yeast[-inTraining0,]
testingY <- as_label(Yeast[-inTraining0, ncol(Yeast)])
table(Yeast[ inTraining0, ncol(Yeast)])
pca <- princomp(tmp[, 2:9], cor=T) # principal components analysis using correlation matrix
pc.comp <- pca$scores
PrincipalComponent1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
PrincipalComponent2 <- -1*pc.comp[,2] # principal component 2 scores (negated for convenience)
clustering.data <- cbind(PrincipalComponent1, PrincipalComponent2)
set.seed(100)
km <- kmeans(clustering.data, 8, iter.max = 30, nstart=30)
#km
km$cluster
plot(PrincipalComponent1, PrincipalComponent2, col=km$cluster)
points(km$centers, pch=16)
aggregate(tmp[, 2:9],by=list(km$cluster),mean)
table(km$cluster, tmp$LocalizationSite)
library(kknn)
cl   <- specClust(clustering.data, centers=8, nn=50, iter.max=100)
#cl
plot(PrincipalComponent1, PrincipalComponent2, col=cl$cluster)
table(cl$cluster, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(cl$cluster),mean)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
#clusterCut
table(clusterCut, tmp$LocalizationSite)
aggregate(tmp[, 2:9],by=list(clusterCut),mean)
plot(PrincipalComponent1, PrincipalComponent2, col=clusterCut)
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Site~., training, update=F) %>%
predict(newdata=testing)
plot(t1)
summary(s1)
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm', 'svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(Site~., training)
#t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
fde1 <- fde(t1$predictions, testingY)
plot_single(fde1, 'score')
store.mtrainer(t1, 'yeast_m8_pre.RData')
saveRDS(testingY, 'yeast_m8_y.RData')
saveRDS(t1, 'yeast_all.RData')
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
fde1 <- fde(t1$predictions, testingY)
plot_single(fde1, 'score')
table(Yeast$Site)
429/(463+429)
521/(4521)
devtools::load_all(".")
library(caret)
tag <- read.csv("tag_data.csv", row.names = 1)
library(caret)
tag <- read.csv("data/tag_data.csv", row.names = 1)
tag <- as.matrix(tag)
## Select only models for regression
regModels <- tag[tag[,"Classification"] == 1,]
all <- 1:nrow(regModels)
## Seed the analysis with the SVM model
start <- grep("(gbm)", rownames(regModels), fixed = TRUE)
pool <- all[all != start]
## Select 4 model models by maximizing the Jaccard
## dissimilarity between sets of models
nextMods <- maxDissim(regModels[start,,drop = FALSE],
regModels[pool, ],
method = "Jaccard",
n = 20)
rownames(regModels)[c(start, nextMods)]
install.packages('foreigh')
?read.arff
install.packages('foreign')
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
library(foreign)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
Seismic <- tmp
Seismic$class <- as.factor(ifelse(Seismic$class == "0", "non-haz", "haz"))
inTraining0 <- createDataPartition(Seismic$class, p = .75, list = FALSE)
training <- Seismic[ inTraining0,]
testing  <- Seismic[-inTraining0,]
testingY <- as_label(Seismic[-inTraining0, ncol(Seismic)])
table(Seismic$class)
tmp <- tmp[,c(4:7,9:18)]
pca <- princomp(tmp, cor=T) # principal components analysis using correlation matrix
?princomp
summary(tmp)
tmp <- tmp[,c(4:7,9:13, 17,18)]
tmp <- tmp[,c(4:7,9:13,17:18)]
Seismic <- tmp
Seismic$class <- as.factor(ifelse(Seismic$class == "0", "non-haz", "haz"))
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
Seismic <- tmp
Seismic$class <- as.factor(ifelse(Seismic$class == "0", "non-haz", "haz"))
inTraining0 <- createDataPartition(Seismic$class, p = .75, list = FALSE)
training <- Seismic[ inTraining0,]
testing  <- Seismic[-inTraining0,]
testingY <- as_label(Seismic[-inTraining0, ncol(Seismic)])
tmp <- tmp[,c(4:7,9:13,17:18)]
pca <- princomp(tmp, cor=T) # principal components analysis using correlation matrix
pc.comp <- pca$scores
PrincipalComponent1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
PrincipalComponent2 <- -1*pc.comp[,2] # principal component 2 scores (negated for convenience)
clustering.data <- cbind(PrincipalComponent1, PrincipalComponent2)
set.seed(100)
km <- kmeans(clustering.data, 8, iter.max = 30, nstart=30)
#km
km$cluster
plot(PrincipalComponent1, PrincipalComponent2, col=km$cluster)
points(km$centers, pch=16)
aggregate(tmp[, 2:9],by=list(km$cluster),mean)
table(km$cluster, tmp$LocalizationSite)
library(kknn)
cl   <- specClust(clustering.data, centers=8, nn=50, iter.max=100)
#cl
plot(PrincipalComponent1, PrincipalComponent2, col=cl$cluster)
table(cl$cluster, tmp$class)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
tmp <- tmp[,c(4:7,9:13,17:18,1)]
pca <- princomp(tmp, cor=T) # principal components analysis using correlation matrix
tmp <- tmp[,c(4:7,9:13,17:18)]
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
Seismic <- tmp
Seismic$class <- as.factor(ifelse(Seismic$class == "0", "non-haz", "haz"))
inTraining0 <- createDataPartition(Seismic$class, p = .75, list = FALSE)
training <- Seismic[ inTraining0,]
testing  <- Seismic[-inTraining0,]
testingY <- as_label(Seismic[-inTraining0, ncol(Seismic)])
tmp <- tmp[,c(4:7,9:13,17:18)]
pca <- princomp(tmp, cor=T) # principal components analysis using correlation matrix
pc.comp <- pca$scores
PrincipalComponent1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
PrincipalComponent2 <- -1*pc.comp[,2] # principal component 2 scores (negated for convenience)
clustering.data <- cbind(PrincipalComponent1, PrincipalComponent2)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
#clusterCut
table(clusterCut, tmp$class)
d_yeast<- dist(clustering.data)
hclusters <- hclust(d_yeast, method = "average")
clusterCut <- cutree(hclusters, 8)
#clusterCut
#table(clusterCut, tmp$class)
#aggregate(tmp[, 2:9],by=list(clusterCut),mean)
plot(PrincipalComponent1, PrincipalComponent2, col=clusterCut)
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Site~., training, update=F) %>%
predict(newdata=testing)
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(class~., training, update=F) %>%
predict(newdata=testing)
#yeast <- read_table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"))
tmp <- read.arff('data/seismic-bumps.arff')
Seismic <- tmp
Seismic$class <- as.factor(ifelse(Seismic$class == "0", "nonhaz", "haz"))
inTraining0 <- createDataPartition(Seismic$class, p = .75, list = FALSE)
training <- Seismic[ inTraining0,]
testing  <- Seismic[-inTraining0,]
testingY <- as_label(Seismic[-inTraining0, ncol(Seismic)])
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(class~., training, update=F) %>%
predict(newdata=testing)
plot(t1)
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
fde1 <- fde(t1$predictions, testingY)
plot_single(fde1, 'score')
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm', 'svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(Site~., training)
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm', 'svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet')) %>%
train(class~., training)
plot(t1)
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
fde1 <- fde(t1$predictions, testingY)
devtools::load_all(".")
fde1 <- fde(t1$predictions, testingY)
devtools::load_all(".")
fde1 <- fde(t1$predictions, testingY)
plot_cor(fde1, legend_flag = T)
names(t1$fitlist)
t1$fitlist <- t1$fitlist[-4]
names(t1$fitlist)
plot_cor(fde1, legend_flag = T)
t1 <- predict(t1, newdata=testing)
store.mtrainer(t1, 'seismic_m8_pre.RData')
saveRDS(testingY, 'seismic_m8_y.RData')
saveRDS(t1, 'seismic_all.RData')
setwd("G:/genomics")
a <- read.csv('HIGGS_H5000.csv')
a <- read.csv('HIGGS_H5000.csv', sep=',')
a <- read.csv2('HIGGS_H5000.csv', header=F, sep=',')
a <- read.csv2('HIGGS_H10000.csv', header=F, sep=',')
a <- read.csv2('HIGGS_H10000.csv', header=F, sep=',', dec ='.')
devtools::load_all(".")
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
Bank <- read.csv('data/bank.csv', sep=';')
inTraining0 <- createDataPartition(Bank$y, p = .75, list = FALSE)
training <- Bank[ inTraining0,]
testing  <- Bank[-inTraining0,]
testingY <- as_label(Bank[-inTraining0, ncol(Bank)])
t1 <- readRDS('UCI/bank_all.RData')
t1
plot(t1)
testingY <- readRDS('UCI/bank_m8_y.RData')
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
install.packages("caretEnsemble")
library(mlbench)
library(FDclassifieR)
#set.seed(1024)
data(Sonar)
inTraining0 <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining0,]
testing  <- Sonar[-inTraining0,]
testingY <- as_label(Sonar[-inTraining0, ncol(Sonar)])
t1 <- mtrainer(c('nnet', 'rda')) %>%
train(Class~., training, update=F) %>%
predict(newdata=testing)
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm', 'LMT', 'mlp', 'nb', 'rf', 'rpart', 'xgbTree')) %>%
train(Class~., training, update=F)
t1 <- t1 %>%
addmodel.mtrainer(c('ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm', 'LMT', 'mlp', 'nb', 'rf', 'rpart', 'xgbTree')) %>%
train(Class~., training, update=F)
install.packages('RWeka')
t1 <- t1 %>%
addmodel.mtrainer(c('LMT', 'mlp', 'nb', 'rf', 'rpart', 'xgbTree', 'ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm')) %>%
train(Class~., training, update=F)
t1 <- t1 %>%
addmodel.mtrainer(c('mlp', 'nb', 'rf', 'rpart', 'xgbTree', 'ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm')) %>%
train(Class~., training, update=F)
plot(t1)
t1 <- t1 %>%
addmodel.mtrainer(c('mlp', 'nb', 'rf', 'rpart', 'xgbTree', 'ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm', 'avNNet', 'glmnet', 'simpls')) %>%
train(Class~., training, update=F)
plot(t1)
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot_cor(fde1, legend_flag = T)
t1 <- readRDS('UCI/bank_all.RData')
testingY <- readRDS('UCI/bank_m8_y.RData')
knitr::opts_chunk$set(echo = TRUE)
library(FDclassifieR)
Bank <- read.csv('data/bank.csv', sep=';')
inTraining0 <- createDataPartition(Bank$y, p = .75, list = FALSE)
training <- Bank[ inTraining0,]
testing  <- Bank[-inTraining0,]
testingY <- as_label(Bank[-inTraining0, ncol(Bank)])
table(Bank$y)
t1 <- mtrainer(c('nnet', 'rda', 'svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet', 'mlp', 'nb', 'rf', 'rpart', 'xgbTree', 'ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm', 'avNNet', 'glmnet', 'simpls', 'xgbLinear','ctree', 'C5.0', 'gbm')) %>%
train(y~., training, update=F)
5593.75/60
plot(t1)
t1 <- predict(t1, newdata=testing)
auclist <- apply(t1$predictions, 2, auc.rank, testingY)
fde1 <- fde(t1$predictions)
fde1 <- predict_performance(fde1, auclist, attr(testingY, 'rho'))
plot(t1)
plot_cor(fde1, legend_flag = T)
fde1 <- fde(t1$predictions, testingY)
plot_single(fde1, 'score')
store.mtrainer(t1, 'bank_m8_pre.RData')
saveRDS(testingY, 'bank_m8_y.RData')
saveRDS(t1, 'bank_all.RData')
plot(t1)
resamples(t1$fitlist)
a <- resamples(t1$fitlist)
a
?resamples
summary(a)
a <- t1$fitlist[1]
a$results
a$nnet$results
a <- resamples(list(nnet=t1$fitlist[[1]], rda=t1$fitlist[[2]]))
plot(a)
a
dotplot(a)
names(t1$fitlist)
str(t1$fitlist)
a <- unlist(t1$fitlist)
saveRDS(t1, 'bank_all.RData')
a <- c('nnet', 'rda', 'svmLinear', 'svmRadial', 'pls', 'earth', 'avNNet', 'mlp', 'nb', 'rf', 'rpart', 'xgbTree', 'ctree', 'C5.0', 'gbm', 'bayesglm', 'earth', 'glm', 'avNNet', 'glmnet', 'simpls', 'xgbLinear','ctree', 'C5.0', 'gbm')
unique(a)
